/*
Copyright 2022 Red Hat Inc.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package tekton

import (
	"context"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"strings"

	"os"
	"time"

	"github.com/ghodss/yaml" // used instead of gopkg.in/yaml.v3 because it treats json tags as yaml tags
	"github.com/go-logr/logr"
	applicationapiv1alpha1 "github.com/konflux-ci/application-api/api/v1alpha1"
	"github.com/konflux-ci/integration-service/api/v1beta2"
	"github.com/konflux-ci/integration-service/loader"
	"github.com/konflux-ci/integration-service/tekton/consts"
	"github.com/konflux-ci/operator-toolkit/metadata"
	tektonv1 "github.com/tektoncd/pipeline/pkg/apis/pipeline/v1"
	resolutionv1beta1 "github.com/tektoncd/pipeline/pkg/apis/resolution/v1beta1"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/util/retry"
	knative "knative.dev/pkg/apis"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
)

// IntegrationPipelineRun is a PipelineRun alias, so we can add new methods to it in this file.
type IntegrationPipelineRun struct {
	tektonv1.PipelineRun
}

// AsPipelineRun casts the IntegrationPipelineRun to PipelineRun, so it can be used in the Kubernetes client.
func (r *IntegrationPipelineRun) AsPipelineRun() *tektonv1.PipelineRun {
	return &r.PipelineRun
}

// NewIntegrationPipelineRun creates an empty PipelineRun in the given namespace. The name will be autogenerated,
// using the prefix passed as an argument to the function.
func NewIntegrationPipelineRun(client client.Client, ctx context.Context, loader loader.ObjectLoader, prefix, namespace string, integrationTestScenario v1beta2.IntegrationTestScenario) (*IntegrationPipelineRun, error) {
	resolverParams := integrationTestScenario.Spec.ResolverRef.Params

	resolver := integrationTestScenario.Spec.ResolverRef.Resolver
	resourceKind := integrationTestScenario.Spec.ResolverRef.ResourceKind
	tektonResolverParams := generateTektonResolverParams(resolverParams)
	if strings.EqualFold(resourceKind, consts.ResourceKindPipeline) || resourceKind == "" {
		return generateIntegrationPipelineRunFromPipelineResolver(prefix, namespace, resolver, tektonResolverParams), nil
	} else if strings.EqualFold(resourceKind, consts.ResourceKindPipelineRun) {
		base64plr, err := getPipelineRunYamlFromPipelineRunResolver(client, ctx, loader, prefix, namespace, resolver, tektonResolverParams)
		if err != nil {
			return nil, err
		}
		return generateIntegrationPipelineRunFromBase64(base64plr, namespace, prefix)
	} else {
		return nil, fmt.Errorf("unrecognized resolver type '%s'", resourceKind)
	}
}

func generateTektonResolverParams(resolverParams []v1beta2.ResolverParameter) []tektonv1.Param {
	tektonResolverParams := []tektonv1.Param{}
	for _, scenarioParam := range resolverParams {
		tektonResolverParam := tektonv1.Param{
			Name: scenarioParam.Name,
			Value: tektonv1.ParamValue{
				Type:      tektonv1.ParamTypeString,
				StringVal: scenarioParam.Value,
			},
		}
		tektonResolverParams = append(tektonResolverParams, tektonResolverParam)
	}
	return tektonResolverParams
}

func generateIntegrationPipelineRunFromPipelineResolver(prefix, namespace, resolver string, resolverParams []tektonv1.Param) *IntegrationPipelineRun {
	pipelineRun := tektonv1.PipelineRun{
		ObjectMeta: metav1.ObjectMeta{
			GenerateName: prefix + "-",
			Namespace:    namespace,
		},
		Spec: tektonv1.PipelineRunSpec{
			PipelineRef: &tektonv1.PipelineRef{
				ResolverRef: tektonv1.ResolverRef{
					Resolver: tektonv1.ResolverName(resolver),
					Params:   resolverParams,
				},
			},
		},
	}

	return &IntegrationPipelineRun{pipelineRun}
}

func getPipelineRunYamlFromPipelineRunResolver(client client.Client, ctx context.Context, loader loader.ObjectLoader, prefix, namespace, resolver string, resolverParams []tektonv1.Param) (string, error) {
	request := resolutionv1beta1.ResolutionRequest{
		ObjectMeta: metav1.ObjectMeta{
			GenerateName: prefix + "-",
			Namespace:    namespace,
			Labels: map[string]string{
				"resolution.tekton.dev/type": resolver,
				"konflux-ci.dev/created-by":  "integration-service", // for backup garbage collection
			},
		},
		Spec: resolutionv1beta1.ResolutionRequestSpec{
			Params: resolverParams,
		},
	}

	err := retry.OnError(retry.DefaultBackoff, func(e error) bool { return true }, func() error {
		return client.Create(ctx, &request)
	})
	if err != nil {
		return "", err
	}

	resolutionRequestName := request.Name
	defer func() {
		_ = retry.OnError(retry.DefaultBackoff, func(e error) bool { return true }, func() error {
			return client.Delete(ctx, &request)
		})
	}()

	// Watch for changes to the ResolutionRequest using efficient polling
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	var resolvedRequest resolutionv1beta1.ResolutionRequest
	errCount := 0

	for {
		select {
		case <-ticker.C:
			resolvedRequest, err = loader.GetResolutionRequest(ctx, client, namespace, resolutionRequestName)
			if err != nil {
				errCount++
				if errCount >= 5 {
					// do not return immediately on error  in case error was a flake in client.Get
					// instead, return after error has been hit repeatedly.  This keeps something like
					// a permissions issue from making the whole service hang
					return "", err
				}
			}

			for _, cond := range resolvedRequest.Status.Conditions {
				if cond.Type == knative.ConditionSucceeded {
					switch cond.Status {
					case corev1.ConditionTrue:
						return resolvedRequest.Status.Data, nil
					case corev1.ConditionFalse:
						return "", fmt.Errorf("resolution for '%s' in namespace '%s' failed: %s", resolutionRequestName, namespace, cond.Message)
					}
				}
			}
		case <-ctx.Done():
			return "", ctx.Err()
		}
	}
}

func generateIntegrationPipelineRunFromBase64(base64plr, namespace, defaultPrefix string) (*IntegrationPipelineRun, error) {
	plrYaml, err := base64.StdEncoding.DecodeString(base64plr)
	if err != nil {
		return nil, err
	}

	var pipelineRun tektonv1.PipelineRun
	err = yaml.Unmarshal(plrYaml, &pipelineRun)
	if err != nil {
		return nil, err
	}

	pipelineRun.Namespace = namespace
	setGenerateNameForPipelineRun(&pipelineRun, defaultPrefix)
	return &IntegrationPipelineRun{pipelineRun}, nil
}

// Ensures that the PipelineRun will use GenerateName rather than name in order
// to avoid collisions. If the resolved PipelineRun has a GenerateName we use
// that.  If it has a Name we convert it to a GenerateName.  Otherwise we use
// the name of the scenario just like with a remote Pipeline
func setGenerateNameForPipelineRun(pipelineRun *tektonv1.PipelineRun, defaultPrefix string) {
	if pipelineRun.GenerateName == "" {
		if pipelineRun.Name != "" {
			pipelineRun.GenerateName = fmt.Sprintf("%s-", pipelineRun.Name)
			pipelineRun.Name = ""
		} else {
			pipelineRun.GenerateName = defaultPrefix
		}
	}
}

// Updates git resolver values parameters with values of params specified in the input map
// updates only exsitings parameters, doens't create new ones
func (iplr *IntegrationPipelineRun) WithUpdatedTestsGitResolver(params map[string]string) *IntegrationPipelineRun {
	//nolint:staticcheck  // Ignore QF1008
	if iplr.Spec.PipelineRef.ResolverRef.Resolver != consts.TektonResolverGit {
		// if the resolver is not git-resolver, we cannot update the git ref
		return iplr
	}

	//nolint:staticcheck  // Ignore QF1008
	for originalParamIndex, originalParam := range iplr.Spec.PipelineRef.ResolverRef.Params {
		if _, ok := params[originalParam.Name]; ok {
			// remeber to use the original index to update the value, we cannot update value given by range directly
			iplr.Spec.PipelineRef.ResolverRef.Params[originalParamIndex].Value.StringVal = params[originalParam.Name]
		}
	}

	return iplr
}

// WithFinalizer adds a Finalizer on the Integration PipelineRun
func (iplr *IntegrationPipelineRun) WithFinalizer(finalizer string) *IntegrationPipelineRun {
	controllerutil.AddFinalizer(iplr, finalizer)

	return iplr
}

// WithExtraParam adds an extra param to the Integration PipelineRun. If the parameter is not part of the Pipeline
// definition, it will be silently ignored.
func (r *IntegrationPipelineRun) WithExtraParam(name string, value tektonv1.ParamValue) *IntegrationPipelineRun {
	r.Spec.Params = append(r.Spec.Params, tektonv1.Param{
		Name:  name,
		Value: value,
	})

	return r
}

// WithExtraParams adds all provided parameters to the Integration PipelineRun.
func (r *IntegrationPipelineRun) WithExtraParams(params []v1beta2.PipelineParameter) *IntegrationPipelineRun {
	for _, param := range params {
		var value tektonv1.ParamValue
		switch {
		case param.Value != "":
			value.StringVal = param.Value
			value.Type = tektonv1.ParamTypeString
		case len(param.Values) > 0:
			value.ArrayVal = param.Values
			value.Type = tektonv1.ParamTypeArray
		default:
			value.Type = tektonv1.ParamTypeObject
		}
		r.WithExtraParam(param.Name, value)
	}

	return r
}

// WithSnapshot adds a param containing the Snapshot as a json string to the integration PipelineRun.
// It also adds the Snapshot name label and copies the Component name label if it exists
func (r *IntegrationPipelineRun) WithSnapshot(snapshot *applicationapiv1alpha1.Snapshot) *IntegrationPipelineRun {
	// We ignore the error here because none should be raised when marshalling the spec of a CRD.
	// If we end up deciding it is useful, we will need to pass the errors through the chain and
	// add something like a `Complete` function that returns the final object and error.
	snapshotString, _ := json.Marshal(snapshot.Spec)

	r.WithExtraParam("SNAPSHOT", tektonv1.ParamValue{
		Type:      tektonv1.ParamTypeString,
		StringVal: string(snapshotString),
	})

	if r.Labels == nil {
		r.Labels = map[string]string{}
	}
	r.Labels[consts.SnapshotNameLabel] = snapshot.Name

	componentLabel, found := snapshot.GetLabels()[consts.ComponentNameLabel]
	if found {
		r.Labels[consts.ComponentNameLabel] = componentLabel
	}

	// copy PipelineRun PAC, build, test and custom annotations/labels from Snapshot to integration test PipelineRun
	prefixes := []string{consts.PipelinesAsCodePrefix, consts.BuildPipelineRunPrefix, consts.TestLabelPrefix, consts.CustomLabelPrefix}
	for _, prefix := range prefixes {
		// Copy labels and annotations prefixed with defined prefix
		_ = metadata.CopyAnnotationsByPrefix(&snapshot.ObjectMeta, &r.ObjectMeta, prefix)
		_ = metadata.CopyLabelsByPrefix(&snapshot.ObjectMeta, &r.ObjectMeta, prefix)
	}

	return r
}

// WithIntegrationLabels adds the type, optional flag and IntegrationTestScenario name as labels to the Integration PipelineRun.
func (r *IntegrationPipelineRun) WithIntegrationLabels(integrationTestScenario *v1beta2.IntegrationTestScenario) *IntegrationPipelineRun {
	if r.Labels == nil {
		r.Labels = map[string]string{}
	}
	r.Labels[consts.PipelinesTypeLabel] = consts.PipelineTypeTest
	r.Labels[consts.ScenarioNameLabel] = integrationTestScenario.Name

	if metadata.HasLabel(integrationTestScenario, consts.OptionalLabel) {
		r.Labels[consts.OptionalLabel] = integrationTestScenario.Labels[consts.OptionalLabel]
	}

	return r
}

// WithIntegrationAnnotations copies the App Studio annotations from the
// IntegrationTestScenario to the PipelineRun
func (r *IntegrationPipelineRun) WithIntegrationAnnotations(its *v1beta2.IntegrationTestScenario) *IntegrationPipelineRun {
	for k, v := range its.GetAnnotations() {
		if strings.Contains(k, "appstudio.openshift.io/") {
			if err := metadata.SetAnnotation(r, k, v); err != nil {
				// this will only happen if we pass IntegrationPipelineRun as nil
				panic(err)
			}
		}
	}

	return r
}

// WithApplication adds the name of application as a label to the Integration PipelineRun.
func (r *IntegrationPipelineRun) WithApplication(application *applicationapiv1alpha1.Application) *IntegrationPipelineRun {
	if r.Labels == nil {
		r.Labels = map[string]string{}
	}
	r.Labels[consts.ApplicationNameLabel] = application.Name

	return r
}

// WithServiceAccount adds the specified service account to the Integration PipelineRun's TaskRunTemplate.
func (r *IntegrationPipelineRun) WithServiceAccount(serviceAccountName string) *IntegrationPipelineRun {
	r.Spec.TaskRunTemplate.ServiceAccountName = serviceAccountName

	return r
}

// WithIntegrationTimeouts fetches the Integration timeouts from either the integrationTestScenario annotations or
// the environment variables and adds them to the integration PipelineRun.
func (r *IntegrationPipelineRun) WithIntegrationTimeouts(integrationTestScenario *v1beta2.IntegrationTestScenario, logger logr.Logger) *IntegrationPipelineRun {
	pipelineTimeoutStr := os.Getenv("PIPELINE_TIMEOUT")
	if metadata.HasAnnotation(integrationTestScenario, v1beta2.PipelineTimeoutAnnotation) {
		pipelineTimeoutStr = integrationTestScenario.Annotations[v1beta2.PipelineTimeoutAnnotation]
	}
	taskTimeoutStr := os.Getenv("TASKS_TIMEOUT")
	if metadata.HasAnnotation(integrationTestScenario, v1beta2.TasksTimeoutAnnotation) {
		taskTimeoutStr = integrationTestScenario.Annotations[v1beta2.TasksTimeoutAnnotation]
	}
	finallyTimeoutStr := os.Getenv("FINALLY_TIMEOUT")
	if metadata.HasAnnotation(integrationTestScenario, v1beta2.FinallyTimeoutAnnotation) {
		finallyTimeoutStr = integrationTestScenario.Annotations[v1beta2.FinallyTimeoutAnnotation]
	}

	r.Spec.Timeouts = &tektonv1.TimeoutFields{}
	if pipelineTimeoutStr != "" {
		pipelineRunTimeout, err := time.ParseDuration(pipelineTimeoutStr)
		if err == nil {
			r.Spec.Timeouts.Pipeline = &metav1.Duration{Duration: pipelineRunTimeout}
		} else {
			logger.Error(err, "failed to parse the PIPELINE_TIMEOUT")
		}
	}
	if taskTimeoutStr != "" {
		taskTimeout, err := time.ParseDuration(taskTimeoutStr)
		if err == nil {
			r.Spec.Timeouts.Tasks = &metav1.Duration{Duration: taskTimeout}
		} else {
			logger.Error(err, "failed to parse the TASKS_TIMEOUT")
		}
	}
	if finallyTimeoutStr != "" {
		finallyTimeout, err := time.ParseDuration(finallyTimeoutStr)
		if err == nil {
			r.Spec.Timeouts.Finally = &metav1.Duration{Duration: finallyTimeout}
		} else {
			logger.Error(err, "failed to parse the FINALLY_TIMEOUT")
		}
	}

	// If the sum of tasks and finally timeout durations is greater than the pipeline timeout duration,
	// increase the pipeline timeout to prevent a pipelineRun validation failure
	if r.Spec.Timeouts.Tasks != nil && r.Spec.Timeouts.Finally != nil && r.Spec.Timeouts.Pipeline != nil &&
		r.Spec.Timeouts.Tasks.Duration+r.Spec.Timeouts.Finally.Duration > r.Spec.Timeouts.Pipeline.Duration {
		r.Spec.Timeouts.Pipeline = &metav1.Duration{Duration: r.Spec.Timeouts.Tasks.Duration + r.Spec.Timeouts.Finally.Duration}
		//nolint:staticcheck  // Ignore QF1008
		logger.Info(fmt.Sprintf("Setting the pipeline timeout for %s to be the sum of tasks + finally: %.1f hours", r.Name,
			r.Spec.Timeouts.Pipeline.Duration.Hours()))
	}

	return r
}
